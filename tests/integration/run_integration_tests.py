"""
é›†æˆæµ‹è¯•è¿è¡Œå™¨
è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import sys
import os
import unittest
import time
import json
from pathlib import Path
from datetime import datetime
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from tests.integration.test_framework import IntegrationTestReporter, run_integration_tests
from tests.integration.test_multi_factor_integration import TestMultiFactorIntegration
from tests.integration.test_technical_analysis_integration import TestTechnicalAnalysisIntegration
from tests.integration.test_market_environment_integration import TestMarketEnvironmentIntegration
from tests.integration.test_risk_management_integration import TestRiskManagementIntegration
from tests.integration.test_end_to_end_workflow import TestEndToEndWorkflow
from tests.integration.test_performance_compatibility import TestPerformanceCompatibility


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("=" * 80)
    print("å·´è²ç‰¹æŠ•èµ„ç³»ç»Ÿ - é›†æˆæµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    reports_dir = Path("reports/integration")
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    # æµ‹è¯•ç±»åˆ—è¡¨
    test_classes = [
        TestMultiFactorIntegration,
        TestTechnicalAnalysisIntegration,
        TestMarketEnvironmentIntegration,
        TestRiskManagementIntegration,
        TestEndToEndWorkflow,
        TestPerformanceCompatibility
    ]
    
    # æµ‹è¯•ç»“æœæ”¶é›†
    all_test_results = []
    test_start_time = time.time()
    
    # è¿è¡Œæ¯ä¸ªæµ‹è¯•ç±»
    for i, test_class in enumerate(test_classes, 1):
        test_name = test_class.__name__
        print(f"\n{'='*20} è¿è¡Œæµ‹è¯• {i}/{len(test_classes)}: {test_name} {'='*20}")
        
        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
        
        # è¿è¡Œæµ‹è¯•
        runner = unittest.TextTestRunner(verbosity=2)
        result = runner.run(suite)
        
        # è®°å½•æµ‹è¯•ç»“æœ
        test_result = {
            "test_class": test_name,
            "tests_run": result.testsRun,
            "failures": len(result.failures),
            "errors": len(result.errors),
            "success_rate": (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0,
            "failure_details": [{"test": str(f[0]), "error": f[1]} for f in result.failures],
            "error_details": [{"test": str(e[0]), "error": e[1]} for e in result.errors]
        }
        
        all_test_results.append(test_result)
        
        print(f"\n{test_name} æµ‹è¯•å®Œæˆ:")
        print(f"  è¿è¡Œæµ‹è¯•: {test_result['tests_run']}")
        print(f"  å¤±è´¥æµ‹è¯•: {test_result['failures']}")
        print(f"  é”™è¯¯æµ‹è¯•: {test_result['errors']}")
        print(f"  æˆåŠŸç‡: {test_result['success_rate']:.2%}")
    
    # è®¡ç®—æ€»ä½“æµ‹è¯•ç»“æœ
    total_tests_run = sum(r["tests_run"] for r in all_test_results)
    total_failures = sum(r["failures"] for r in all_test_results)
    total_errors = sum(r["errors"] for r in all_test_results)
    total_success_rate = (total_tests_run - total_failures - total_errors) / total_tests_run if total_tests_run > 0 else 0
    total_time = time.time() - test_start_time
    
    print(f"\n{'='*80}")
    print("é›†æˆæµ‹è¯•æ±‡æ€»")
    print(f"{'='*80}")
    print(f"æ€»è¿è¡Œæµ‹è¯•: {total_tests_run}")
    print(f"æ€»å¤±è´¥æµ‹è¯•: {total_failures}")
    print(f"æ€»é”™è¯¯æµ‹è¯•: {total_errors}")
    print(f"æ€»ä½“æˆåŠŸç‡: {total_success_rate:.2%}")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ±‡æ€»æŠ¥å‘Š
    summary_report = {
        "report_type": "integration_test_summary",
        "timestamp": datetime.now().isoformat(),
        "test_summary": {
            "total_tests_run": total_tests_run,
            "total_failures": total_failures,
            "total_errors": total_errors,
            "total_success_rate": total_success_rate,
            "total_time": total_time
        },
        "test_results": all_test_results
    }
    
    summary_file = reports_dir / f"integration_test_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)
    
    # è¯¦ç»†æŠ¥å‘Š
    detailed_report = {
        "report_type": "integration_test_detailed",
        "timestamp": datetime.now().isoformat(),
        "test_environment": {
            "python_version": sys.version,
            "platform": sys.platform,
            "working_directory": os.getcwd()
        },
        "test_classes": [cls.__name__ for cls in test_classes],
        "test_results": all_test_results,
        "performance_summary": generate_performance_summary(all_test_results),
        "compatibility_summary": generate_compatibility_summary(all_test_results)
    }
    
    detailed_file = reports_dir / f"integration_test_detailed_{timestamp}.json"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_report, f, ensure_ascii=False, indent=2)
    
    # HTMLæŠ¥å‘Š
    html_file = reports_dir / f"integration_test_report_{timestamp}.html"
    generate_html_report(detailed_report, html_file)
    
    print(f"\næŠ¥å‘Šæ–‡ä»¶:")
    print(f"  æ±‡æ€»æŠ¥å‘Š: {summary_file}")
    print(f"  è¯¦ç»†æŠ¥å‘Š: {detailed_file}")
    print(f"  HTMLæŠ¥å‘Š: {html_file}")
    
    # è¿”å›é€€å‡ºç 
    if total_failures > 0 or total_errors > 0:
        print(f"\nâš ï¸  é›†æˆæµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥å¤±è´¥å’Œé”™è¯¯è¯¦æƒ…")
        return 1
    else:
        print(f"\nâœ… æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        return 0


def generate_performance_summary(test_results):
    """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
    performance_summary = {
        "test_classes_with_performance": [],
        "performance_issues": []
    }
    
    for result in test_results:
        test_class = result["test_class"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½ç›¸å…³çš„å¤±è´¥
        performance_failures = [
            f for f in result["failure_details"] 
            if "æ€§èƒ½" in f["error"] or "performance" in f["error"].lower()
        ]
        
        performance_errors = [
            e for e in result["error_details"] 
            if "æ€§èƒ½" in e["error"] or "performance" in e["error"].lower()
        ]
        
        if performance_failures or performance_errors:
            performance_summary["test_classes_with_performance"].append(test_class)
            performance_summary["performance_issues"].extend([
                {"test_class": test_class, "type": "failure", "detail": f}
                for f in performance_failures
            ])
            performance_summary["performance_issues"].extend([
                {"test_class": test_class, "type": "error", "detail": e}
                for e in performance_errors
            ])
    
    return performance_summary


def generate_compatibility_summary(test_results):
    """ç”Ÿæˆå…¼å®¹æ€§æ‘˜è¦"""
    compatibility_summary = {
        "test_classes_with_compatibility": [],
        "compatibility_issues": []
    }
    
    for result in test_results:
        test_class = result["test_class"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¼å®¹æ€§ç›¸å…³çš„å¤±è´¥
        compatibility_failures = [
            f for f in result["failure_details"] 
            if "å…¼å®¹" in f["error"] or "compatibility" in f["error"].lower()
        ]
        
        compatibility_errors = [
            e for e in result["error_details"] 
            if "å…¼å®¹" in e["error"] or "compatibility" in e["error"].lower()
        ]
        
        if compatibility_failures or compatibility_errors:
            compatibility_summary["test_classes_with_compatibility"].append(test_class)
            compatibility_summary["compatibility_issues"].extend([
                {"test_class": test_class, "type": "failure", "detail": f}
                for f in compatibility_failures
            ])
            compatibility_summary["compatibility_issues"].extend([
                {"test_class": test_class, "type": "error", "detail": e}
                for e in compatibility_errors
            ])
    
    return compatibility_summary


def generate_html_report(report_data, output_file):
    """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å·´è²ç‰¹æŠ•èµ„ç³»ç»Ÿ - é›†æˆæµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1, h2, h3 {{
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }}
        h1 {{ font-size: 2.5em; }}
        h2 {{ font-size: 1.8em; margin-top: 30px; }}
        h3 {{ font-size: 1.4em; margin-top: 25px; }}
        .summary {{
            background-color: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }}
        .test-result {{
            border: 1px solid #ddd;
            margin: 15px 0;
            padding: 15px;
            border-radius: 5px;
        }}
        .success {{ border-left: 5px solid #27ae60; }}
        .failure {{ border-left: 5px solid #e74c3c; }}
        .error {{ border-left: 5px solid #e67e22; }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }}
        .stat-card {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            text-align: center;
            border: 1px solid #e9ecef;
        }}
        .stat-value {{
            font-size: 2em;
            font-weight: bold;
            color: #2c3e50;
        }}
        .stat-label {{
            color: #7f8c8d;
            font-size: 0.9em;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
        .pass {{ color: #27ae60; }}
        .fail {{ color: #e74c3c; }}
        .error {{ color: #e67e22; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ å·´è²ç‰¹æŠ•èµ„ç³»ç»Ÿ - é›†æˆæµ‹è¯•æŠ¥å‘Š</h1>
        
        <div class="summary">
            <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
            <div class="stats">
                <div class="stat-card">
                    <div class="stat-value">{report_data['test_results'][0]['tests_run'] if report_data['test_results'] else 0}</div>
                    <div class="stat-label">æ€»æµ‹è¯•æ•°</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">{sum(r['failures'] for r in report_data['test_results'])}</div>
                    <div class="stat-label">å¤±è´¥æ•°</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">{sum(r['errors'] for r in report_data['test_results'])}</div>
                    <div class="stat-label">é”™è¯¯æ•°</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">{report_data['test_results'][0]['success_rate']:.1% if report_data['test_results'] else '0%'}</div>
                    <div class="stat-label">æˆåŠŸç‡</div>
                </div>
            </div>
        </div>
        
        <h2>ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
        {generate_test_results_html(report_data['test_results'])}
        
        <h2>âš¡ æ€§èƒ½æ‘˜è¦</h2>
        {generate_performance_html(report_data.get('performance_summary', {}))}
        
        <h2>ğŸ”„ å…¼å®¹æ€§æ‘˜è¦</h2>
        {generate_compatibility_html(report_data.get('compatibility_summary', {}))}
        
        <div class="summary">
            <p><strong>æŠ¥å‘Šç”Ÿæˆæ—¶é—´:</strong> {report_data['timestamp']}</p>
            <p><strong>æµ‹è¯•ç¯å¢ƒ:</strong> {report_data.get('test_environment', {}).get('platform', 'Unknown')}</p>
        </div>
    </div>
</body>
</html>
    """
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)


def generate_test_results_html(test_results):
    """ç”Ÿæˆæµ‹è¯•ç»“æœçš„HTML"""
    html = ""
    for result in test_results:
        status_class = "success" if result['failures'] == 0 and result['errors'] == 0 else "failure"
        if result['errors'] > 0:
            status_class = "error"
        
        html += f"""
        <div class="test-result {status_class}">
            <h3>{result['test_class']}</h3>
            <div class="stats">
                <div class="stat-card">
                    <div class="stat-value">{result['tests_run']}</div>
                    <div class="stat-label">è¿è¡Œæµ‹è¯•</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value fail">{result['failures']}</div>
                    <div class="stat-label">å¤±è´¥</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value error">{result['errors']}</div>
                    <div class="stat-label">é”™è¯¯</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value pass">{result['success_rate']:.1%}</div>
                    <div class="stat-label">æˆåŠŸç‡</div>
                </div>
            </div>
        </div>
        """
    
    return html


def generate_performance_html(performance_summary):
    """ç”Ÿæˆæ€§èƒ½æ‘˜è¦çš„HTML"""
    if not performance_summary.get('test_classes_with_performance'):
        return "<p>âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡</p>"
    
    html = "<div class='test-result failure'>"
    html += "<h3>âš ï¸ æ€§èƒ½é—®é¢˜</h3>"
    html += "<table><tr><th>æµ‹è¯•ç±»</th><th>é—®é¢˜ç±»å‹</th><th>è¯¦æƒ…</th></tr>"
    
    for issue in performance_summary.get('performance_issues', []):
        html += f"""
        <tr>
            <td>{issue['test_class']}</td>
            <td>{issue['type']}</td>
            <td>{issue['detail'][:100]}...</td>
        </tr>
        """
    
    html += "</table></div>"
    return html


def generate_compatibility_html(compatibility_summary):
    """ç”Ÿæˆå…¼å®¹æ€§æ‘˜è¦çš„HTML"""
    if not compatibility_summary.get('test_classes_with_compatibility'):
        return "<p>âœ… æ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•é€šè¿‡</p>"
    
    html = "<div class='test-result failure'>"
    html += "<h3>âš ï¸ å…¼å®¹æ€§é—®é¢˜</h3>"
    html += "<table><tr><th>æµ‹è¯•ç±»</th><th>é—®é¢˜ç±»å‹</th><th>è¯¦æƒ…</th></tr>"
    
    for issue in compatibility_summary.get('compatibility_issues', []):
        html += f"""
        <tr>
            <td>{issue['test_class']}</td>
            <td>{issue['type']}</td>
            <td>{issue['detail'][:100]}...</td>
        </tr>
        """
    
    html += "</table></div>"
    return html


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)