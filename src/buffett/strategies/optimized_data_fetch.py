"""
ä¼˜åŒ–åçš„æ•°æ®è·å–ç­–ç•¥ - åŸºäºAKShareæŠ€èƒ½çš„æ™ºèƒ½APIè°ƒç”¨ç®¡ç†

è¿™ä¸ªæ¨¡å—å®ç°äº†åŸºäºæŠ€èƒ½åŒ–AKShareè°ƒç”¨çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¸¥æ ¼æ§åˆ¶APIè°ƒç”¨æ¬¡æ•°ï¼š
1. æ‰¹é‡æ•°æ®è·å–ä¼˜å…ˆ
2. æ™ºèƒ½ç¼“å­˜ç®¡ç†
3. æ•°æ®æºåˆ†å±‚è°ƒç”¨
4. è¯·æ±‚å»é‡å’Œåˆå¹¶
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Set, Tuple
import pandas as pd
import asyncio
import time
import hashlib
import json
import pickle
import os
from datetime import datetime, date, timedelta
from dataclasses import dataclass, field
from collections import defaultdict
import logging

from ..interfaces.providers import IDataProvider
from ..models.stock import StockInfo, DividendData
from ..exceptions.data import DataFetchError

logger = logging.getLogger(__name__)


@dataclass
class APIRequestTracker:
    """APIè¯·æ±‚è·Ÿè¸ªå™¨ï¼Œç”¨äºé¢‘ç‡æ§åˆ¶"""
    total_requests: int = 0
    requests_by_source: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    last_request_times: Dict[str, datetime] = field(default_factory=dict)

    def can_request(self, source: str, min_interval: int = 30) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘èµ·è¯·æ±‚"""
        now = datetime.now()
        last_time = self.last_request_times.get(source)

        if last_time and (now - last_time).seconds < min_interval:
            return False
        return True

    def record_request(self, source: str):
        """è®°å½•ä¸€æ¬¡è¯·æ±‚"""
        self.total_requests += 1
        self.requests_by_source[source] += 1
        self.last_request_times[source] = datetime.now()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯·æ±‚ç»Ÿè®¡"""
        return {
            'total_requests': self.total_requests,
            'requests_by_source': dict(self.requests_by_source),
            'last_request_times': {
                source: time.isoformat() for source, time in self.last_request_times.items()
            }
        }


@dataclass
class BatchRequest:
    """æ‰¹é‡è¯·æ±‚ç®¡ç†"""
    symbols: Set[str] = field(default_factory=set)
    data_types: Set[str] = field(default_factory=set)
    priority: int = 1
    created_at: datetime = field(default_factory=datetime.now)

    def add_symbol(self, symbol: str, data_type: str = 'basic'):
        """æ·»åŠ è‚¡ç¥¨åˆ°æ‰¹é‡è¯·æ±‚"""
        self.symbols.add(symbol)
        self.data_types.add(data_type)

    def merge_with(self, other: 'BatchRequest'):
        """åˆå¹¶ä¸¤ä¸ªæ‰¹é‡è¯·æ±‚"""
        self.symbols.update(other.symbols)
        self.data_types.update(other.data_types)
        self.priority = max(self.priority, other.priority)


class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ - æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ä¸åŒçš„TTL"""

    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = cache_dir
        self._ensure_cache_dir()

        # ä¸åŒæ•°æ®ç±»å‹çš„TTLè®¾ç½® - å»¶é•¿ç¼“å­˜æ—¶é—´å‡å°‘APIè°ƒç”¨
        self.ttl_settings = {
            'market_overview': timedelta(hours=2),        # å¸‚åœºæ¦‚è§ˆæ•°æ®2å°æ—¶ (å‡å°‘è°ƒç”¨)
            'individual_stock': timedelta(hours=6),      # ä¸ªè‚¡è¯¦æƒ…6å°æ—¶
            'dividend_data': timedelta(hours=48),        # è‚¡æ¯æ•°æ®48å°æ—¶
            'historical_data': timedelta(days=14),       # å†å²æ•°æ®14å¤©
            'basic_info': timedelta(hours=4),            # åŸºæœ¬ä¿¡æ¯4å°æ—¶
        }

        # è‚¡ç¥¨ä»£ç æ˜ å°„ç¼“å­˜ - é¿å…é‡å¤è·å–å¸‚åœºæ•°æ®æ¥æŸ¥æ‰¾ä»£ç 
        self.symbol_mapping_file = os.path.join(cache_dir, "symbol_mapping.json")
        self.symbol_mapping = self._load_symbol_mapping()

    def _ensure_cache_dir(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        import os
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, "market"), exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, "individual"), exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, "dividends"), exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, "historical"), exist_ok=True)

    def _get_cache_path(self, data_type: str, key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        if data_type == 'market_overview':
            return os.path.join(self.cache_dir, "market", f"overview.pkl")
        elif data_type == 'individual_stock':
            return os.path.join(self.cache_dir, "individual", f"{key}.pkl")
        elif data_type == 'dividend_data':
            return os.path.join(self.cache_dir, "dividends", f"{key}.pkl")
        elif data_type == 'historical_data':
            return os.path.join(self.cache_dir, "historical", f"{key}.pkl")
        else:
            return os.path.join(self.cache_dir, f"{key}.json")

    def _is_cache_valid(self, cache_path: str, data_type: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False

        ttl = self.ttl_settings.get(data_type, timedelta(hours=1))
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        return datetime.now() - file_time < ttl

    def get_cached_data(self, data_type: str, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_path = self._get_cache_path(data_type, key)

        if self._is_cache_valid(cache_path, data_type):
            try:
                if cache_path.endswith('.json'):
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                else:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
            except Exception as e:
                logger.debug(f"ç¼“å­˜è¯»å–å¤±è´¥ {data_type}:{key} - {e}")
                try:
                    os.remove(cache_path)
                except:
                    pass
        return None

    def cache_data(self, data_type: str, key: str, data: Any):
        """ç¼“å­˜æ•°æ®"""
        cache_path = self._get_cache_path(data_type, key)
        try:
            if cache_path.endswith('.json'):
                with open(cache_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            else:
                with open(cache_path, 'wb') as f:
                    pickle.dump(data, f)
            logger.debug(f"æ•°æ®å·²ç¼“å­˜ {data_type}:{key}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥ {data_type}:{key} - {e}")

    def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜ - ä¿ç•™æ›´é•¿æ—¶é—´å‡å°‘APIè°ƒç”¨"""
        now = datetime.now()
        for root, dirs, files in os.walk(self.cache_dir):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    # å»¶é•¿ä¿ç•™æ—¶é—´åˆ°3å¤©ï¼Œå‡å°‘é‡æ–°è·å–çš„éœ€æ±‚
                    if now - file_time > timedelta(days=3):
                        os.remove(file_path)
                except:
                    pass

    def _load_symbol_mapping(self) -> Dict[str, str]:
        """åŠ è½½è‚¡ç¥¨ä»£ç æ˜ å°„"""
        try:
            if os.path.exists(self.symbol_mapping_file):
                with open(self.symbol_mapping_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.debug(f"åŠ è½½è‚¡ç¥¨ä»£ç æ˜ å°„å¤±è´¥: {e}")
        return {}

    def save_symbol_mapping(self):
        """ä¿å­˜è‚¡ç¥¨ä»£ç æ˜ å°„"""
        try:
            with open(self.symbol_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(self.symbol_mapping, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.debug(f"ä¿å­˜è‚¡ç¥¨ä»£ç æ˜ å°„å¤±è´¥: {e}")

    def get_mapped_symbol(self, standard_symbol: str, market_data: pd.DataFrame = None) -> Optional[str]:
        """
        è·å–æ˜ å°„åçš„è‚¡ç¥¨ä»£ç ï¼Œé¿å…é‡å¤æŸ¥æ‰¾

        Args:
            standard_symbol: æ ‡å‡†æ ¼å¼çš„è‚¡ç¥¨ä»£ç  (å¦‚ 000001.SZ)
            market_data: å¸‚åœºæ•°æ®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ˜ å°„ç¼“å­˜
        """
        # é¦–å…ˆæ£€æŸ¥æ˜ å°„ç¼“å­˜
        if standard_symbol in self.symbol_mapping:
            return self.symbol_mapping[standard_symbol]

        # å¦‚æœæ²¡æœ‰ç¼“å­˜ä¸”æ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œè¿”å›None
        if market_data is None or market_data.empty:
            return None

        # åœ¨å¸‚åœºæ•°æ®ä¸­æŸ¥æ‰¾
        code = standard_symbol.split('.')[0]  # æå–6ä½ä»£ç 

        # å°è¯•æŒ‰åç§°åŒ¹é…
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„åŒ¹é…é€»è¾‘ï¼Œæ¯”å¦‚å·²çŸ¥è‚¡ç¥¨åç§°çš„ç›´æ¥æ˜ å°„
        name_mappings = {
            '000001': 'å¹³å®‰é“¶è¡Œ',
            '600036': 'æ‹›å•†é“¶è¡Œ',
            '600000': 'æµ¦å‘é“¶è¡Œ',
            '601318': 'ä¸­å›½å¹³å®‰',
            '000002': 'ä¸‡ç§‘A',
            '600519': 'è´µå·èŒ…å°',
            '000858': 'äº”ç²®æ¶²',
        }

        target_name = name_mappings.get(code)
        if target_name:
            matched_rows = market_data[market_data['åç§°'] == target_name]
            if not matched_rows.empty:
                mapped_code = matched_rows.iloc[0]['ä»£ç ']
                self.symbol_mapping[standard_symbol] = mapped_code
                self.save_symbol_mapping()
                return mapped_code

        # å¦‚æœåç§°åŒ¹é…å¤±è´¥ï¼ŒæŒ‰ä»£ç å‰ç¼€å’Œä½ç½®åŒ¹é…
        if code.startswith('6'):  # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
            # åœ¨ä¸Šæµ·è‚¡ç¥¨ä¸­æŸ¥æ‰¾
            sh_stocks = market_data[market_data['ä»£ç '].str.startswith(('6', '9'), na=False)]
            if not sh_stocks.empty:
                # ç®€å•çš„é¡ºåºæ˜ å°„
                sh_stocks_sorted = sh_stocks.sort_values('ä»£ç ')
                index = int(code[3:]) % len(sh_stocks_sorted)
                mapped_code = sh_stocks_sorted.iloc[index]['ä»£ç ']
                self.symbol_mapping[standard_symbol] = mapped_code
                self.save_symbol_mapping()
                return mapped_code
        else:  # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
            # åœ¨æ·±åœ³è‚¡ç¥¨ä¸­æŸ¥æ‰¾
            sz_stocks = market_data[market_data['ä»£ç '].str.startswith(('0', '2', '3'), na=False)]
            if not sz_stocks.empty:
                # ç®€å•çš„é¡ºåºæ˜ å°„
                sz_stocks_sorted = sz_stocks.sort_values('ä»£ç ')
                index = int(code[3:]) % len(sz_stocks_sorted)
                mapped_code = sz_stocks_sorted.iloc[index]['ä»£ç ']
                self.symbol_mapping[standard_symbol] = mapped_code
                self.save_symbol_mapping()
                return mapped_code

        return None


class OptimizedDataFetcher:
    """ä¼˜åŒ–çš„æ•°æ®è·å–å™¨ - ä¸¥æ ¼æ§åˆ¶APIè°ƒç”¨æ¬¡æ•°"""

    def __init__(self, enable_cache: bool = True, cache_ttl_hours: int = 24):
        self.enable_cache = enable_cache
        self.cache = SmartCache() if enable_cache else None
        self.request_tracker = APIRequestTracker()

        # æ‰¹é‡è¯·æ±‚é˜Ÿåˆ—
        self.batch_requests = []
        self.batch_size_limit = 50  # æœ€å¤§æ‰¹é‡å¤§å°

        # é¢‘ç‡æ§åˆ¶è®¾ç½® - æ›´ä¸¥æ ¼çš„é™åˆ¶
        self.rate_limits = {
            'sina': 120,     # æ–°æµªè´¢ç» - 2åˆ†é’Ÿé—´éš” (é¿å…è¢«å°)
            'xueqiu': 10,    # é›ªçƒ - 10ç§’é—´éš”
            'tencent': 30,   # è…¾è®¯è¯åˆ¸ - 30ç§’é—´éš”
        }

    async def fetch_market_overview(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–å¸‚åœºæ¦‚è§ˆæ•°æ® - ä½¿ç”¨market_overviewæŠ€èƒ½ (ä¸¥æ ¼æ§åˆ¶è°ƒç”¨é¢‘ç‡)

        è¿™æ˜¯æ‰€æœ‰æ•°æ®è·å–çš„èµ·ç‚¹ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰è‚¡ç¥¨çš„åŸºæœ¬æ•°æ®
        ä¸¥æ ¼é™åˆ¶è°ƒç”¨é¢‘ç‡ï¼Œé¿å…IPè¢«å°
        """
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜ - è¿™æ˜¯æœ€é‡è¦çš„ä¼˜åŒ–
        if not force_refresh and self.cache:
            cached_data = self.cache.get_cached_data('market_overview', 'all')
            if cached_data is not None and not cached_data.empty:
                logger.info(f"âœ… ä»ç¼“å­˜è·å–å¸‚åœºæ¦‚è§ˆæ•°æ®: {len(cached_data)} åªè‚¡ç¥¨ (é¿å…APIè°ƒç”¨)")
                return cached_data

        # ä¸¥æ ¼çš„é¢‘ç‡æ§åˆ¶ - å¢åŠ åˆ°2åˆ†é’Ÿé—´éš”
        min_interval = self.rate_limits.get('sina', 120)
        if not self.request_tracker.can_request('sina', min_interval):
            last_request = self.request_tracker.last_request_times.get('sina')
            if last_request:
                wait_time = min_interval - (datetime.now() - last_request).seconds
                if wait_time > 0:
                    logger.warning(f"â° å¸‚åœºæ¦‚è§ˆè¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’ (é¿å…è¢«å°IP)")
                    await asyncio.sleep(wait_time)

        try:
            logger.info("ğŸ”„ [é‡è¦] è°ƒç”¨ Market Overview æŠ€èƒ½è·å–å¸‚åœºæ•°æ®...")
            logger.warning("âš ï¸  æ³¨æ„ï¼šæ­¤æ“ä½œä¼šå¤§é‡è°ƒç”¨æ–°æµªè´¢ç»APIï¼Œè¯·å‹¿é¢‘ç¹è¿è¡Œ")

            import akshare as ak

            # è°ƒç”¨skill: "akshare" (market_overview)
            market_data = ak.stock_zh_a_spot()

            if market_data.empty:
                raise DataFetchError("å¸‚åœºæ¦‚è§ˆæ•°æ®ä¸ºç©º")

            # è®°å½•è¯·æ±‚
            self.request_tracker.record_request('sina')

            # å¼ºåˆ¶ç¼“å­˜æ•°æ® - å»¶é•¿ç¼“å­˜æ—¶é—´
            if self.cache:
                self.cache.cache_data('market_overview', 'all', market_data)
                logger.info(f"ğŸ’¾ å¸‚åœºæ¦‚è§ˆæ•°æ®å·²ç¼“å­˜ï¼Œ2å°æ—¶å†…æ— éœ€å†æ¬¡è°ƒç”¨API")

            logger.info(f"âœ… æˆåŠŸè·å–å¸‚åœºæ¦‚è§ˆæ•°æ®: {len(market_data)} åªè‚¡ç¥¨")
            logger.warning(f"âš ï¸  ä¸‹æ¬¡è°ƒç”¨éœ€ç­‰å¾… {min_interval//60} åˆ†é’Ÿï¼Œé¿å…è¢«å°IP")

            return market_data

        except Exception as e:
            logger.error(f"âŒ å¸‚åœºæ¦‚è§ˆæ•°æ®è·å–å¤±è´¥: {e}")
            raise DataFetchError(f"Failed to fetch market overview: {str(e)}")

    async def fetch_stocks_batch(self, symbols: List[str], data_types: List[str] = None) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ® - ä¼˜å…ˆä½¿ç”¨å¸‚åœºæ¦‚è§ˆæ•°æ®ï¼Œå¿…è¦æ—¶è¡¥å……ä¸ªè‚¡è¯¦æƒ…

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            data_types: éœ€è¦çš„æ•°æ®ç±»å‹ ['basic', 'detailed', 'dividend']
        """
        if data_types is None:
            data_types = ['basic', 'detailed']

        results = {}

        # ç¬¬ä¸€æ­¥ï¼šä»å¸‚åœºæ¦‚è§ˆè·å–åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if 'basic' in data_types:
            logger.info(f"ğŸ“Š ä»å¸‚åœºæ¦‚è§ˆè·å– {len(symbols)} åªè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯...")
            market_data = await self.fetch_market_overview()

            # ä»å¸‚åœºæ•°æ®ä¸­æå–ç›®æ ‡è‚¡ç¥¨
            for symbol in symbols:
                code = symbol.split('.')[0]
                stock_rows = market_data[market_data['ä»£ç '] == code]

                if not stock_rows.empty:
                    stock_data = stock_rows.iloc[0].to_dict()
                    results[symbol] = {
                        'basic': {
                            'symbol': symbol,
                            'code': code,
                            'name': stock_data.get('åç§°', ''),
                            'current_price': self._parse_number(stock_data.get('æœ€æ–°ä»·')),
                            'market_cap': self._parse_number(stock_data.get('æ€»å¸‚å€¼')),
                            'circulating_market_cap': self._parse_number(stock_data.get('æµé€šå¸‚å€¼')),
                            'pe_ratio': self._parse_number(stock_data.get('å¸‚ç›ˆç‡-åŠ¨æ€')),
                            'pb_ratio': self._parse_number(stock_data.get('å¸‚å‡€ç‡')),
                            'turnover_rate': self._parse_number(stock_data.get('æ¢æ‰‹ç‡')),
                            'volume': self._parse_number(stock_data.get('æˆäº¤é‡')),
                            'amount': self._parse_number(stock_data.get('æˆäº¤é¢')),
                            'change_pct': self._parse_number(stock_data.get('æ¶¨è·Œå¹…')),
                            'data_source': 'market_overview'
                        }
                    }
                    logger.debug(f"ä»å¸‚åœºæ¦‚è§ˆè·å– {symbol} åŸºæœ¬ä¿¡æ¯æˆåŠŸ")
                else:
                    logger.warning(f"å¸‚åœºæ¦‚è§ˆä¸­æœªæ‰¾åˆ°è‚¡ç¥¨ {symbol}")

        # ç¬¬äºŒæ­¥ï¼šè·å–è¯¦ç»†ä¸ªè‚¡ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ä¸”ä¸å·²æœ‰ï¼‰ - åªåœ¨çœŸæ­£éœ€è¦æ—¶è°ƒç”¨
        if 'detailed' in data_types:
            logger.info(f"ğŸ” è·å– {len(symbols)} åªè‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯ (ä»…å¿…è¦æ—¶è°ƒç”¨API)...")

            for i, symbol in enumerate(symbols):
                # æ›´ä¸¥æ ¼çš„é¢‘ç‡æ§åˆ¶ - é¿å…é›ªçƒAPIé¢‘ç¹è°ƒç”¨
                if not self.request_tracker.can_request('xueqiu', 10):
                    wait_time = 10 - (datetime.now() - self.request_tracker.last_request_times.get('xueqiu', datetime.min)).seconds
                    if wait_time > 0:
                        logger.warning(f"â° é›ªçƒAPIé¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’")
                        await asyncio.sleep(wait_time)

                try:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰è¶³å¤Ÿçš„åŸºæœ¬ä¿¡æ¯ï¼Œå¯ä»¥è·³è¿‡è¯¦ç»†è°ƒç”¨
                    if symbol in results and 'basic' in results[symbol]:
                        basic_info = results[symbol]['basic']
                        # å¦‚æœåŸºæœ¬ä¿¡æ¯è¶³å¤Ÿå®Œæ•´ï¼Œå¯ä»¥è·³è¿‡è¯¦ç»†APIè°ƒç”¨
                        if (basic_info.get('current_price') and
                            basic_info.get('market_cap') and
                            basic_info.get('pe_ratio') is not None):
                            logger.debug(f"â­ï¸  {symbol} åŸºæœ¬ä¿¡æ¯å®Œæ•´ï¼Œè·³è¿‡è¯¦ç»†APIè°ƒç”¨")
                            continue

                    detailed_data = await self._fetch_individual_stock_detailed(symbol)
                    if detailed_data:
                        if symbol not in results:
                            results[symbol] = {}
                        results[symbol]['detailed'] = detailed_data
                        logger.debug(f"âœ… è·å– {symbol} è¯¦ç»†ä¿¡æ¯æˆåŠŸ")
                    else:
                        logger.debug(f"âš ï¸  {symbol} è¯¦ç»†ä¿¡æ¯ä¸ºç©º")

                except Exception as e:
                    logger.warning(f"âŒ è·å– {symbol} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                    continue

                # æ¯10ä¸ªè‚¡ç¥¨åå¢åŠ é¢å¤–å»¶è¿Ÿï¼Œé¿å…è¢«é™åˆ¶
                if i > 0 and i % 10 == 0:
                    logger.info(f"ğŸ“Š å·²å¤„ç† {i}/{len(symbols)} åªè‚¡ç¥¨ï¼Œæš‚åœ5ç§’é¿å…APIé™åˆ¶...")
                    await asyncio.sleep(5)

        logger.info(f"âœ… æ‰¹é‡æ•°æ®è·å–å®Œæˆ: {len(results)}/{len(symbols)} åªè‚¡ç¥¨")
        return results

    async def _fetch_individual_stock_detailed(self, symbol: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ä¸ªè‚¡è¯¦ç»†ä¿¡æ¯ - ä½¿ç”¨individual_stockæŠ€èƒ½

        åªåœ¨çœŸæ­£éœ€è¦æ—¶è°ƒç”¨ï¼Œé¿å…é¢‘ç¹APIè¯·æ±‚
        """
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached_data = self.cache.get_cached_data('individual_stock', symbol)
            if cached_data:
                logger.debug(f"ä»ç¼“å­˜è·å– {symbol} è¯¦ç»†ä¿¡æ¯")
                return cached_data

        try:
            # è°ƒç”¨skill: "akshare" (individual_stock)
            import akshare as ak

            code = symbol.split('.')[0]
            xq_symbol = f"SH{code}" if code.startswith('6') else f"SZ{code}"

            stock_data = ak.stock_individual_spot_xq(symbol=xq_symbol)

            if stock_data.empty:
                return None

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            data_dict = dict(zip(stock_data['item'], stock_data['value']))

            # æå–å…³é”®ä¿¡æ¯
            detailed_info = {
                'symbol': symbol,
                'code': code,
                'name': data_dict.get('åç§°', ''),
                'current_price': self._parse_number(data_dict.get('ç°ä»·')),
                'pe_ratio_dynamic': self._parse_number(data_dict.get('å¸‚ç›ˆç‡(åŠ¨)')),
                'pe_ratio_static': self._parse_number(data_dict.get('å¸‚ç›ˆç‡(é™)')),
                'pe_ratio_ttm': self._parse_number(data_dict.get('å¸‚ç›ˆç‡(TTM)')),
                'pb_ratio': self._parse_number(data_dict.get('å¸‚å‡€ç‡')),
                'eps': self._parse_number(data_dict.get('æ¯è‚¡æ”¶ç›Š')),
                'book_value': self._parse_number(data_dict.get('æ¯è‚¡å‡€èµ„äº§')),
                'dividend_ttm': self._parse_number(data_dict.get('è‚¡æ¯(TTM)')),
                'dividend_yield': self._parse_number(data_dict.get('è‚¡æ¯ç‡(TTM)')),
                'week_52_high': self._parse_number(data_dict.get('52å‘¨æœ€é«˜')),
                'week_52_low': self._parse_number(data_dict.get('52å‘¨æœ€ä½')),
                'market_cap': self._parse_number(data_dict.get('æµé€šå€¼')),
                'shares_outstanding': self._parse_number(data_dict.get('æµé€šè‚¡')),
                'volume': self._parse_number(data_dict.get('æˆäº¤é‡')),
                'turnover_rate': self._parse_number(data_dict.get('å‘¨è½¬ç‡')),
                'data_source': 'xueqiu_detailed',
                'fetch_time': datetime.now().isoformat()
            }

            # è®°å½•è¯·æ±‚
            self.request_tracker.record_request('xueqiu')

            # ç¼“å­˜æ•°æ®
            if self.cache:
                self.cache.cache_data('individual_stock', symbol, detailed_info)

            return detailed_info

        except Exception as e:
            logger.debug(f"ä¸ªè‚¡è¯¦ç»†ä¿¡æ¯è·å–å¤±è´¥ {symbol}: {e}")
            return None

    async def fetch_dividend_data_batch(self, symbols: List[str]) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡æ¯æ•°æ® - ä½¿ç”¨historical_dataæŠ€èƒ½è·å–åˆ†çº¢å†å²

        ä¼˜åŒ–çš„æ‰¹é‡è‚¡æ¯æ•°æ®è·å–ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
        """
        results = {}

        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œæ£€æŸ¥ç¼“å­˜
        uncached_symbols = []
        for symbol in symbols:
            if self.cache:
                cached_data = self.cache.get_cached_data('dividend_data', symbol)
                if cached_data is not None and not cached_data.empty:
                    results[symbol] = cached_data
                    logger.debug(f"ä»ç¼“å­˜è·å– {symbol} è‚¡æ¯æ•°æ®")
                else:
                    uncached_symbols.append(symbol)
            else:
                uncached_symbols.append(symbol)

        logger.info(f"ğŸ“ˆ æ‰¹é‡è·å– {len(uncached_symbols)} åªè‚¡ç¥¨çš„è‚¡æ¯æ•°æ®...")

        # æ‰¹é‡è·å–æœªç¼“å­˜çš„è‚¡æ¯æ•°æ® - ä¸¥æ ¼æ§åˆ¶é¢‘ç‡
        for i, symbol in enumerate(uncached_symbols):
            # é¢‘ç‡æ§åˆ¶ï¼šæ¯5ä¸ªè‚¡ç¥¨æš‚åœ5ç§’ï¼Œé¿å…è¢«å°
            if i > 0 and i % 5 == 0:
                logger.info(f"ğŸ“Š å·²è·å– {i}/{len(uncached_symbols)} åªè‚¡ç¥¨è‚¡æ¯æ•°æ®ï¼Œæš‚åœ5ç§’...")
                await asyncio.sleep(5)

            # åŸºç¡€å»¶è¿Ÿ
            if i > 0:
                await asyncio.sleep(1)  # æ¯ä¸ªè‚¡ç¥¨é—´1ç§’é—´éš”

            try:
                dividend_data = await self._fetch_single_dividend_data(symbol)
                if dividend_data is not None and not dividend_data.empty:
                    results[symbol] = dividend_data
                    logger.debug(f"è·å– {symbol} è‚¡æ¯æ•°æ®æˆåŠŸ: {len(dividend_data)} æ¡è®°å½•")
                else:
                    # è¿”å›ç©ºDataFrameè€Œä¸æ˜¯Noneï¼Œä¿æŒä¸€è‡´æ€§
                    results[symbol] = pd.DataFrame()
                    logger.debug(f"{symbol} æ— è‚¡æ¯æ•°æ®")

            except Exception as e:
                logger.warning(f"è·å– {symbol} è‚¡æ¯æ•°æ®å¤±è´¥: {e}")
                results[symbol] = pd.DataFrame()

        logger.info(f"âœ… è‚¡æ¯æ•°æ®æ‰¹é‡è·å–å®Œæˆ: {len(results)}/{len(symbols)} åªè‚¡ç¥¨")
        return results

    async def _fetch_single_dividend_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """è·å–å•ä¸ªè‚¡ç¥¨çš„è‚¡æ¯æ•°æ®"""
        try:
            import akshare as ak

            code = symbol.split('.')[0]

            # ä¼˜å…ˆä½¿ç”¨ç¨³å®šçš„history_detailæ¥å£
            dividend_data = ak.stock_history_dividend_detail(symbol=code)

            if dividend_data.empty:
                return pd.DataFrame()

            # å¤„ç†è‚¡æ¯æ•°æ®
            processed_data = self._process_dividend_data(symbol, dividend_data)

            # è®°å½•è¯·æ±‚
            self.request_tracker.record_request('akshare')

            # ç¼“å­˜æ•°æ®
            if self.cache:
                self.cache.cache_data('dividend_data', symbol, processed_data)

            return processed_data

        except Exception as e:
            logger.debug(f"è‚¡æ¯æ•°æ®è·å–å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _process_dividend_data(self, symbol: str, dividend_data: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†è‚¡æ¯æ•°æ®æ ¼å¼"""
        try:
            processed = pd.DataFrame()
            processed['symbol'] = [symbol] * len(dividend_data)

            # æå–å¹´åº¦ä¿¡æ¯
            years = []
            for announce_date in dividend_data['å…¬å‘Šæ—¥æœŸ']:
                try:
                    if isinstance(announce_date, str):
                        year = int(announce_date.split('-')[0])
                    else:
                        year = announce_date.year
                    years.append(year)
                except Exception:
                    years.append(0)
            processed['year'] = years

            # å¤„ç†ç°é‡‘è‚¡æ¯
            cash_dividends = []
            for amount in dividend_data['æ´¾æ¯']:
                if pd.isna(amount):
                    cash_dividends.append(0.0)
                else:
                    try:
                        cash_dividends.append(float(amount) / 10.0)  # æ¯10è‚¡æ´¾æ¯
                    except (ValueError, TypeError):
                        cash_dividends.append(0.0)
            processed['cash_dividend'] = cash_dividends

            # å¤„ç†è‚¡ç¥¨è‚¡æ¯
            stock_dividends = []
            for send_stock, bonus_stock in zip(dividend_data['é€è‚¡'], dividend_data['è½¬å¢']):
                total = 0.0
                try:
                    if pd.notna(send_stock):
                        total += float(send_stock) / 10.0
                    if pd.notna(bonus_stock):
                        total += float(bonus_stock) / 10.0
                except (ValueError, TypeError):
                    pass
                stock_dividends.append(total)
            processed['stock_dividend'] = stock_dividends

            # å¤„ç†æ—¥æœŸ
            processed['record_date'] = pd.to_datetime(dividend_data['è‚¡æƒç™»è®°æ—¥'], errors='coerce')
            processed['ex_dividend_date'] = pd.to_datetime(dividend_data['é™¤æƒé™¤æ¯æ—¥'], errors='coerce')
            processed['payment_date'] = pd.to_datetime(dividend_data['çº¢è‚¡ä¸Šå¸‚æ—¥'], errors='coerce')
            processed['is_annual_report'] = True  # é»˜è®¤ä¸ºå¹´æŠ¥

            return processed

        except Exception as e:
            logger.error(f"è‚¡æ¯æ•°æ®å¤„ç†å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    async def fetch_historical_data_batch(self, symbols: List[str],
                                         start_date: date, end_date: date,
                                         adjust: str = "hfq") -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–å†å²æ•°æ® - ä½¿ç”¨historical_dataæŠ€èƒ½

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹ ("qfq"å‰å¤æƒ, "hfq"åå¤æƒ, ""ä¸å¤æƒ)
        """
        results = {}

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_{adjust}"

        logger.info(f"ğŸ“Š æ‰¹é‡è·å– {len(symbols)} åªè‚¡ç¥¨çš„å†å²æ•°æ® ({cache_key})...")

        for i, symbol in enumerate(symbols):
            # æ£€æŸ¥ç¼“å­˜
            cache_symbol_key = f"{symbol}_{cache_key}"
            if self.cache:
                cached_data = self.cache.get_cached_data('historical_data', cache_symbol_key)
                if cached_data is not None and not cached_data.empty:
                    results[symbol] = cached_data
                    logger.debug(f"ä»ç¼“å­˜è·å– {symbol} å†å²æ•°æ®")
                    continue

            # é¢‘ç‡æ§åˆ¶ï¼šå†å²æ•°æ®è¯·æ±‚ç›¸å¯¹å®½æ¾
            if i > 0 and i % 3 == 0:  # æ¯3ä¸ªè‚¡ç¥¨æš‚åœ
                await asyncio.sleep(1)

            try:
                historical_data = await self._fetch_single_historical_data(
                    symbol, start_date, end_date, adjust
                )
                if historical_data is not None and not historical_data.empty:
                    results[symbol] = historical_data
                    # ç¼“å­˜æ•°æ®
                    if self.cache:
                        self.cache.cache_data('historical_data', cache_symbol_key, historical_data)
                    logger.debug(f"è·å– {symbol} å†å²æ•°æ®æˆåŠŸ: {len(historical_data)} æ¡è®°å½•")

            except Exception as e:
                logger.warning(f"è·å– {symbol} å†å²æ•°æ®å¤±è´¥: {e}")

        logger.info(f"âœ… å†å²æ•°æ®æ‰¹é‡è·å–å®Œæˆ: {len(results)}/{len(symbols)} åªè‚¡ç¥¨")
        return results

    async def _fetch_single_historical_data(self, symbol: str,
                                           start_date: date, end_date: date,
                                           adjust: str) -> Optional[pd.DataFrame]:
        """è·å–å•ä¸ªè‚¡ç¥¨çš„å†å²æ•°æ®"""
        try:
            import akshare as ak

            code = symbol.split('.')[0]
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')

            # è°ƒç”¨skill: "akshare" (historical_data)
            hist_data = ak.stock_zh_a_hist_tx(
                symbol=code,
                start_date=start_str,
                end_date=end_str,
                adjust=adjust
            )

            if hist_data.empty:
                return pd.DataFrame()

            # æ·»åŠ symbolåˆ—
            hist_data['symbol'] = symbol

            # è®°å½•è¯·æ±‚
            self.request_tracker.record_request('tencent')

            return hist_data

        except Exception as e:
            logger.debug(f"å†å²æ•°æ®è·å–å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def _parse_number(self, value) -> Optional[float]:
        """å®‰å…¨è§£ææ•°å­—"""
        if value is None or value == '' or value == '-':
            return None
        try:
            if isinstance(value, str):
                value = value.replace(',', '').replace('ï¼Œ', '').replace('--', '0')
            return float(value)
        except (ValueError, TypeError):
            return None

    def get_api_stats(self) -> Dict[str, Any]:
        """è·å–APIè°ƒç”¨ç»Ÿè®¡"""
        return self.request_tracker.get_stats()

    def clear_cache(self, pattern: str = None):
        """æ¸…ç†ç¼“å­˜"""
        if self.cache:
            if pattern:
                # æ¸…ç†ç‰¹å®šæ¨¡å¼çš„ç¼“å­˜
                import os
                for root, dirs, files in os.walk(self.cache.cache_dir):
                    for file in files:
                        if pattern in file:
                            try:
                                os.remove(os.path.join(root, file))
                            except:
                                pass
            else:
                self.cache.clear_expired_cache()

    def optimize_request_sequence(self, symbols: List[str],
                                 data_types: List[str]) -> List[Tuple[str, List[str]]]:
        """
        ä¼˜åŒ–è¯·æ±‚åºåˆ—ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°

        Returns:
            List of (method_name, symbols) tuples
        """
        sequence = []

        # ç­–ç•¥1: å¦‚æœéœ€è¦åŸºæœ¬ä¿¡æ¯ä¸”è‚¡ç¥¨æ•°é‡å¤šï¼Œä¼˜å…ˆä½¿ç”¨å¸‚åœºæ¦‚è§ˆ
        if 'basic' in data_types and len(symbols) > 10:
            sequence.append(('fetch_market_overview', symbols))

        # ç­–ç•¥2: å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œæ‰¹é‡è·å–
        if 'detailed' in data_types:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§è¯·æ±‚è¿‡å¤š
            batch_size = self.batch_size_limit
            for i in range(0, len(symbols), batch_size):
                batch = symbols[i:i + batch_size]
                sequence.append(('fetch_stocks_batch', batch))

        # ç­–ç•¥3: è‚¡æ¯æ•°æ®å•ç‹¬æ‰¹é‡å¤„ç†
        if 'dividend' in data_types:
            batch_size = min(self.batch_size_limit, 20)  # è‚¡æ¯æ•°æ®æ‰¹é‡æ›´å°
            for i in range(0, len(symbols), batch_size):
                batch = symbols[i:i + batch_size]
                sequence.append(('fetch_dividend_data_batch', batch))

        return sequence


# å‘åå…¼å®¹çš„åŒ…è£…å™¨
class OptimizedAKShareStrategy(OptimizedDataFetcher):
    """å‘åå…¼å®¹çš„AKShareç­–ç•¥åŒ…è£…å™¨"""

    def __init__(self, proxy: Optional[str] = None, timeout: int = 30,
                 cache_ttl_hours: int = 24, enable_cache: bool = True):
        super().__init__(enable_cache, cache_ttl_hours)
        self.proxy = proxy
        self.timeout = timeout
        self._connection_tested = False

    async def fetch_all_stocks(self) -> pd.DataFrame:
        """å…¼å®¹æ¥å£ï¼šè·å–æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨"""
        market_data = await self.fetch_market_overview()

        # è½¬æ¢ä¸ºæ—§æ ¼å¼
        stock_list = market_data[['ä»£ç ', 'åç§°']].copy()
        stock_list.columns = ['code', 'name']
        stock_list['symbol'] = stock_list['code'] + '.SH'  # ç®€åŒ–å¤„ç†

        return stock_list

    async def fetch_stock_basic_info(self, symbol: str) -> Optional[Dict[str, Any]]:
        """å…¼å®¹æ¥å£ï¼šè·å–å•ä¸ªè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        results = await self.fetch_stocks_batch([symbol], ['basic', 'detailed'])

        if symbol in results:
            stock_data = results[symbol]

            # åˆå¹¶åŸºæœ¬ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
            basic_info = stock_data.get('basic', {})
            detailed_info = stock_data.get('detailed', {})

            # åˆå¹¶æ•°æ®ï¼Œdetailedä¿¡æ¯ä¼˜å…ˆ
            merged_info = {**basic_info, **detailed_info}
            return merged_info if merged_info else None

        return None

    async def fetch_dividend_data(self, symbol: str) -> pd.DataFrame:
        """å…¼å®¹æ¥å£ï¼šè·å–å•ä¸ªè‚¡ç¥¨è‚¡æ¯æ•°æ®"""
        results = await self.fetch_dividend_data_batch([symbol])
        return results.get(symbol, pd.DataFrame())

    async def fetch_price_data(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """å…¼å®¹æ¥å£ï¼šè·å–å•ä¸ªè‚¡ç¥¨å†å²æ•°æ®"""
        results = await self.fetch_historical_data_batch([symbol], start_date, end_date)
        return results.get(symbol, pd.DataFrame())

    async def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        if self._connection_tested:
            return True

        try:
            # å°è¯•è·å–å°‘é‡æ•°æ®æµ‹è¯•è¿æ¥
            market_data = await self.fetch_market_overview()
            self._connection_tested = not market_data.empty
            return self._connection_tested
        except Exception:
            self._connection_tested = False
            return False

    def get_strategy_name(self) -> str:
        """è·å–ç­–ç•¥åç§°"""
        return "OptimizedAKShare"